# ------------------------------------------------------------------------------------
#  Performance Summary (Reusable Workflow) (GoFortress)
#
#  Purpose: Generate a comprehensive performance summary report for the entire
#  workflow run, including timing metrics, cache statistics, and test results.
#
#  Maintainer: @mrz1836
#
# ------------------------------------------------------------------------------------

name: GoFortress (Performance Summary)

on:
  workflow_call:
    inputs:
      benchmarks-result:
        description: "Benchmarks job result"
        required: false
        type: string
        default: "skipped"
      start-epoch:
        description: "Workflow start epoch time"
        required: true
        type: string
      start-time:
        description: "Workflow start time"
        required: true
        type: string
      setup-result:
        description: "Setup job result"
        required: true
        type: string
      test-magex-result:
        description: "Test MAGE-X job result"
        required: true
        type: string
      security-result:
        description: "Security job result"
        required: true
        type: string
      code-quality-result:
        description: "Code quality job result"
        required: true
        type: string
      pre-commit-result:
        description: "Pre-commit checks job result"
        required: true
        type: string
      test-suite-result:
        description: "Test suite job result"
        required: true
        type: string
      release-result:
        description: "Result of the release job"
        required: false
        type: string
        default: "skipped"
      status-check-result:
        description: "Result of the status-check job"
        required: false
        type: string
        default: "skipped"
      test-matrix:
        description: "Test matrix JSON"
        required: true
        type: string
      env-json:
        description: "JSON string of environment variables"
        required: true
        type: string
      primary-runner:
        description: "Primary runner OS"
        required: true
        type: string

# Security: Restrictive default permissions with job-level overrides for least privilege access
permissions:
  contents: read

jobs:
  # ----------------------------------------------------------------------------------
  # Performance Summary Report
  # ----------------------------------------------------------------------------------
  performance-summary:
    name: 📊 Performance Summary Report
    runs-on: ${{ inputs.primary-runner }}
    steps:
      # ————————————————————————————————————————————————————————————————
      # Parse environment variables
      # ————————————————————————————————————————————————————————————————
      - name: 🔧 Parse environment variables
        env:
          ENV_JSON: ${{ inputs.env-json }}
        run: |
          echo "📋 Setting environment variables..."
          echo "$ENV_JSON" | jq -r 'to_entries | .[] | "\(.key)=\(.value)"' | while IFS='=' read -r key value; do
            echo "$key=$value" >> $GITHUB_ENV
          done

      # ————————————————————————————————————————————————————————————————
      # Download all statistics artifacts
      # ————————————————————————————————————————————————————————————————
      - name: 📥 Download performance artifacts
        if: always()
        uses: actions/download-artifact@634f93cb2916e3fdff6788551b99b062d0335ce0 # v5.0.0
        with:
          pattern: "*-stats-*"
          path: ./performance-artifacts/

      - name: 📥 Download test result artifacts for failure analysis
        if: always()
        uses: actions/download-artifact@634f93cb2916e3fdff6788551b99b062d0335ce0 # v5.0.0
        continue-on-error: true
        with:
          pattern: "test-results-*"
          path: ./test-artifacts/

      # ————————————————————————————————————————————————————————————————
      # Flatten performance artifacts for processing
      # ————————————————————————————————————————————————————————————————
      - name: 🗂️ Flatten performance and test artifacts
        if: always()
        run: |
          echo "🗂️ Flattening downloaded artifacts..."

          # Find all JSON files in subdirectories and move them to current directory
          if [ -d "./performance-artifacts/" ]; then
            find ./performance-artifacts/ -name "*.json" -type f | while read -r file; do
              filename=$(basename "$file")
              echo "Moving $file to ./$filename"
              cp "$file" "./$filename"
            done

            # List all flattened files for debugging
            echo "📋 Available stats files:"
            ls -la *-stats-*.json 2>/dev/null || echo "No stats files found"
          else
            echo "⚠️ No performance-artifacts directory found"
          fi

          # Process test artifacts for additional failure details
          if [ -d "./test-artifacts/" ]; then
            find ./test-artifacts/ -name "*.json" -type f | while read -r file; do
              filename=$(basename "$file")
              echo "Moving test artifact $file to ./$filename"
              cp "$file" "./$filename"
            done

            echo "📋 Available test artifact files:"
            ls -la test-failures*.json 2>/dev/null || echo "No test failure files found"
          else
            echo "⚠️ No test-artifacts directory found"
          fi

      # ————————————————————————————————————————————————————————————————
      # Calculate timing metrics
      # ————————————————————————————————————————————————————————————————
      - name: ⏱️ Calculate Timing Metrics
        id: calculate-timing
        run: |
          # Calculate total duration
          START_EPOCH=${{ inputs.start-epoch }}
          END_EPOCH=$(date +%s)
          TOTAL_DURATION=$((END_EPOCH - START_EPOCH))
          TOTAL_MINUTES=$((TOTAL_DURATION / 60))
          TOTAL_SECONDS=$((TOTAL_DURATION % 60))

          # Store as outputs for later use
          echo "total_minutes=$TOTAL_MINUTES" >> $GITHUB_OUTPUT
          echo "total_seconds=$TOTAL_SECONDS" >> $GITHUB_OUTPUT
          echo "total_duration=$TOTAL_DURATION" >> $GITHUB_OUTPUT

      # ————————————————————————————————————————————————————————————————
      # Initialize performance report
      # ————————————————————————————————————————————————————————————————
      - name: 📝 Initialize Performance Report
        id: init-report
        run: |
          # Create the initial performance summary structure
          {
            echo "## 📊 Workflow Performance Metrics"
            echo ""
            echo "### ⏱️ Overall Timing"
            echo "| Metric | Value |"
            echo "|--------|-------|"
            echo "| **Total Duration** | ${{ steps.calculate-timing.outputs.total_minutes }}m ${{ steps.calculate-timing.outputs.total_seconds }}s |"
            echo "| **Start Time** | ${{ inputs.start-time }} |"
            echo "| **End Time** | $(date -u +"%Y-%m-%dT%H:%M:%SZ") |"
            echo "| **Workflow** | ${{ github.workflow }} |"
            echo "| **Run Number** | ${{ github.run_number }} |"
            echo "| **Trigger** | ${{ github.event_name }} |"
            echo "| **Source** | ${{ github.event.pull_request.head.repo.full_name == github.repository && 'Internal' || 'Fork' }} |"
            echo ""
          } > performance-report.md

      # ————————————————————————————————————————————————————————————————
      # Process cache statistics
      # ————————————————————————————————————————————————————————————————
      - name: 💾 Process Cache Statistics
        id: process-cache
        run: |
          # Process cache statistics if available
          if compgen -G "cache-stats-*.json" >/dev/null 2>&1; then
            {
              echo ""
              echo "### 💾 Cache Performance"
              echo "| OS | Go Version | Module Cache | Build Cache | Module Size | Build Size |"
              echo "|----|------------|--------------|-------------|-------------|------------|"
            } >> performance-report.md

            TOTAL_CACHE_HITS=0
            TOTAL_CACHE_ATTEMPTS=0

            for stats_file in cache-stats-*.json; do
              if [ -f "$stats_file" ]; then
                OS=$(jq -r '.os' "$stats_file")
                GO_VER=$(jq -r '.go_version' "$stats_file")
                GOMOD_HIT=$(jq -r '.gomod_cache_hit' "$stats_file")
                GOBUILD_HIT=$(jq -r '.gobuild_cache_hit' "$stats_file")
                GOMOD_SIZE=$(jq -r '.cache_size_gomod' "$stats_file")
                GOBUILD_SIZE=$(jq -r '.cache_size_gobuild' "$stats_file")

                GOMOD_ICON=$([[ "$GOMOD_HIT" == "true" ]] && echo "✅ Hit" || echo "❌ Miss")
                GOBUILD_ICON=$([[ "$GOBUILD_HIT" == "true" ]] && echo "✅ Hit" || echo "❌ Miss")

                echo "| $OS | $GO_VER | $GOMOD_ICON | $GOBUILD_ICON | $GOMOD_SIZE | $GOBUILD_SIZE |" >> performance-report.md

                [[ "$GOMOD_HIT" == "true" ]] && TOTAL_CACHE_HITS=$((TOTAL_CACHE_HITS + 1))
                [[ "$GOBUILD_HIT" == "true" ]] && TOTAL_CACHE_HITS=$((TOTAL_CACHE_HITS + 1))
                TOTAL_CACHE_ATTEMPTS=$((TOTAL_CACHE_ATTEMPTS + 2))
              fi
            done
          fi

      # ————————————————————————————————————————————————————————————————
      # Process benchmark statistics
      # ————————————————————————————————————————————————————————————————
      - name: 🏃 Process Benchmark Statistics
        id: process-benchmarks
        run: |
          # Process benchmark statistics if available
          if compgen -G "benchmark-stats-*.json" >/dev/null 2>&1; then
            {
              echo ""
              echo ""
              echo "### 🏃 Benchmark Performance"
            } >> performance-report.md

            # Get benchmark mode from the first stats file
            BENCH_MODE="normal"
            for stats_file in benchmark-stats-*.json; do
              if [ -f "$stats_file" ]; then
                BENCH_MODE=$(jq -r '.benchmark_mode // "normal"' "$stats_file")
                break
              fi
            done

            {
              echo ""
              echo "**Mode**: \`$BENCH_MODE\` $(case "$BENCH_MODE" in quick) echo "(Quick 50ms runs)" ;; full) echo "(Comprehensive 10s runs)" ;; *) echo "(Normal 100ms runs)" ;; esac)"
              echo ""
              echo "| Benchmark Suite | Duration | Benchmarks | Status |"
              echo "|-----------------|----------|------------|--------|"
            } >> performance-report.md

            for stats_file in benchmark-stats-*.json; do
              if [ -f "$stats_file" ]; then
                NAME=$(jq -r '.name' "$stats_file")
                DURATION=$(jq -r '.duration_seconds' "$stats_file")
                BENCHMARK_COUNT=$(jq -r '.benchmark_count' "$stats_file")
                STATUS=$(jq -r '.status' "$stats_file")

                DURATION_MIN=$((DURATION / 60))
                DURATION_SEC=$((DURATION % 60))
                STATUS_ICON=$([[ "$STATUS" == "success" ]] && echo "✅" || echo "❌")

                echo "| $NAME | ${DURATION_MIN}m ${DURATION_SEC}s | $BENCHMARK_COUNT | $STATUS_ICON |" >> performance-report.md
              fi
            done

            # Display detailed benchmark results
            {
              echo ""
              echo "<details>"
              echo "<summary>Detailed Benchmark Results</summary>"
              echo ""
            } >> performance-report.md

            for stats_file in benchmark-stats-*.json; do
              if [ -f "$stats_file" ]; then
                NAME=$(jq -r '.name' "$stats_file")
                BENCHMARK_SUMMARY=$(jq -r '.benchmark_summary' "$stats_file")
                if [ -n "$BENCHMARK_SUMMARY" ] && [ "$BENCHMARK_SUMMARY" != "null" ]; then
                  {
                    echo "#### $NAME"
                    echo "$BENCHMARK_SUMMARY"
                    echo ""
                  } >> performance-report.md
                fi
              fi
            done

            echo "</details>" >> performance-report.md
          fi

      # ————————————————————————————————————————————————————————————————
      # Process test statistics
      # ————————————————————————————————————————————————————————————————
      - name: 🧪 Process Test Statistics
        id: process-tests
        run: |
          # Process test statistics if available
          if compgen -G "test-stats-*.json" >/dev/null 2>&1; then
            {
              echo ""
              echo ""
              echo "### 🧪 Test Execution Performance"
              echo "| Test Suite | Mode | Duration | Tests | Failed | Packages | Status | Race | Coverage |"
              echo "|------------|------|----------|-------|--------|----------|--------|------|----------|"
            } >> performance-report.md

            # Initialize totals for summary
            TOTAL_TESTS=0
            TOTAL_FAILURES=0
            TOTAL_AFFECTED_PACKAGES=0
            SUITE_COUNT=0

            for stats_file in test-stats-*.json; do
              if [ -f "$stats_file" ]; then
                NAME=$(jq -r '.name' "$stats_file")
                DURATION=$(jq -r '.duration_seconds' "$stats_file")
                TEST_COUNT=$(jq -r '.test_count' "$stats_file")
                STATUS=$(jq -r '.status' "$stats_file")
                RACE_ENABLED=$(jq -r '.race_enabled' "$stats_file")
                COVERAGE_ENABLED=$(jq -r '.coverage_enabled' "$stats_file")

                # New enhanced fields
                TEST_MODE=$(jq -r '.test_mode // "unknown"' "$stats_file")
                SUITE_FAILURES=$(jq -r '.total_failures // 0' "$stats_file")
                AFFECTED_PACKAGES=$(jq -r '.affected_packages // 0' "$stats_file")

                DURATION_MIN=$((DURATION / 60))
                DURATION_SEC=$((DURATION % 60))

                COVERAGE_ICON=$([[ "$COVERAGE_ENABLED" == "true" ]] && echo "✅" || echo "❌")
                RACE_ICON=$([[ "$RACE_ENABLED" == "true" ]] && echo "✅" || echo "❌")
                STATUS_ICON=$([[ "$STATUS" == "success" ]] && echo "✅" || echo "❌")

                # Show package count or dash
                PACKAGE_DISPLAY=$([[ "$AFFECTED_PACKAGES" -gt 0 ]] && echo "$AFFECTED_PACKAGES" || echo "-")

                echo "| $NAME | $TEST_MODE | ${DURATION_MIN}m ${DURATION_SEC}s | $TEST_COUNT | $SUITE_FAILURES | $PACKAGE_DISPLAY | $STATUS_ICON | $RACE_ICON | $COVERAGE_ICON |" >> performance-report.md

                # Accumulate totals
                TOTAL_TESTS=$((TOTAL_TESTS + TEST_COUNT))
                TOTAL_FAILURES=$((TOTAL_FAILURES + SUITE_FAILURES))
                TOTAL_AFFECTED_PACKAGES=$((TOTAL_AFFECTED_PACKAGES + AFFECTED_PACKAGES))
                SUITE_COUNT=$((SUITE_COUNT + 1))
              fi
            done

            # Store totals as outputs for later use
            echo "total_tests=$TOTAL_TESTS" >> $GITHUB_OUTPUT
            echo "total_failures=$TOTAL_FAILURES" >> $GITHUB_OUTPUT
            echo "suite_count=$SUITE_COUNT" >> $GITHUB_OUTPUT

            # Add test failure analysis if any failures exist
            if [[ $TOTAL_FAILURES -gt 0 ]]; then
              {
                echo ""
                echo ""
                echo "### ❌ Test Failure Analysis"
                echo "**Total Failures**: $TOTAL_FAILURES across $SUITE_COUNT test suite(s)"
                echo ""
                echo "#### 📊 Failures by Test Suite:"
              } >> performance-report.md

              for stats_file in test-stats-*.json; do
                if [ -f "$stats_file" ]; then
                  SUITE_NAME=$(jq -r '.name' "$stats_file")
                  SUITE_FAILURES=$(jq -r '.total_failures // 0' "$stats_file")
                  SUITE_PACKAGES=$(jq -r '.affected_packages // 0' "$stats_file")

                  if [[ $SUITE_FAILURES -gt 0 ]]; then
                    echo "- **$SUITE_NAME**: $SUITE_FAILURES failures across $SUITE_PACKAGES packages" >> performance-report.md
                  fi
                fi
              done

              {
                echo ""
                echo "<details>"
                echo "<summary>🔍 Top Failed Tests (click to expand)</summary>"
                echo ""
                echo "| Test Name | Package | Duration | Suite |"
                echo "|-----------|---------|----------|-------|"
              } >> performance-report.md

              # Extract detailed failure information from all suites
              FAILURE_COUNT=0
              for stats_file in test-stats-*.json; do
                if [ -f "$stats_file" ] && [[ $FAILURE_COUNT -lt 20 ]]; then
                  SUITE_NAME=$(jq -r '.name' "$stats_file")
                  FAILURE_DETAILS=$(jq -r '.failure_details // null' "$stats_file")

                  if [[ "$FAILURE_DETAILS" != "null" ]] && [[ "$FAILURE_DETAILS" != "[]" ]]; then
                    echo "$FAILURE_DETAILS" | jq -r --arg suite "$SUITE_NAME" \
                      '.[] | "| \(.Test) | \(.Package | split("/") | .[-1] // .[-2] // .) | \(.Duration // "unknown")s | \($suite) |"' 2>/dev/null | \
                      head -10 >> performance-report.md || true
                    FAILURE_COUNT=$((FAILURE_COUNT + 10))
                  fi
                fi
              done

              {
                echo ""
                echo "</details>"
              } >> performance-report.md

              # Add error details section for failed tests
              HAS_ERROR_OUTPUT=false
              for stats_file in test-stats-*.json; do
                if [ -f "$stats_file" ]; then
                  FAILURE_DETAILS=$(jq -r '.failure_details // null' "$stats_file")
                  if [[ "$FAILURE_DETAILS" != "null" ]] && [[ "$FAILURE_DETAILS" != "[]" ]]; then
                    # Check if any failure has non-empty output
                    HAS_OUTPUT=$(echo "$FAILURE_DETAILS" | jq -r 'map(select(.Output != "" and .Output != null)) | length > 0' 2>/dev/null)
                    if [[ "$HAS_OUTPUT" == "true" ]]; then
                      HAS_ERROR_OUTPUT=true
                      break
                    fi
                  fi
                fi
              done

              if [[ "$HAS_ERROR_OUTPUT" == "true" ]]; then
                {
                  echo ""
                  echo ""
                  echo "### 📝 Test Error Messages"
                  echo ""
                } >> performance-report.md

                ERROR_COUNT=0
                for stats_file in test-stats-*.json; do
                  if [ -f "$stats_file" ] && [[ $ERROR_COUNT -lt 10 ]]; then
                    SUITE_NAME=$(jq -r '.name' "$stats_file")
                    FAILURE_DETAILS=$(jq -r '.failure_details // null' "$stats_file")

                    if [[ "$FAILURE_DETAILS" != "null" ]] && [[ "$FAILURE_DETAILS" != "[]" ]]; then
                      # Display failures with non-empty outputs using smart truncation
                      echo "$FAILURE_DETAILS" | jq -r --arg suite "$SUITE_NAME" \
                        '.[] | select(.Output != "" and .Output != null) |
                        "#### \(.Test) (\(.Package | split("/") | .[-1] // .[-2] // .))\n\n```\n\(.Output | if length > 1500 then .[0:1500] + "\n... (truncated)" else . end)\n```\n"' 2>/dev/null | \
                        head -c 4000 >> performance-report.md || true
                      ERROR_COUNT=$((ERROR_COUNT + 3))
                    fi
                  fi
                done
              fi
            fi
          fi

      # ————————————————————————————————————————————————————————————————
      # Add test configuration and LOC summary
      # ————————————————————————————————————————————————————————————————
      - name: 🎛️ Add Test Configuration Section
        id: add-test-config
        run: |
          # Add test output configuration section
          if compgen -G "test-stats-*.json" >/dev/null 2>&1; then
            {
              echo ""
              echo ""
              echo "### 🎛️ Test Output Configuration"
              echo "| Configuration | Value | Description |"
              echo "|---------------|-------|-------------|"
              echo "| **Mode** | ${TEST_OUTPUT_MODE:-SMART} | Test output strategy |"
              echo "| **Smart Threshold** | ${TEST_OUTPUT_SMART_THRESHOLD:-500} tests | Switch to FAILURES_ONLY above this |"
              echo "| **Detail Count** | ${TEST_FAILURE_DETAIL_COUNT:-50} | Failures shown with details |"
              echo "| **Annotations** | ${TEST_FAILURE_ANNOTATION_COUNT:-10} | GitHub annotations limit |"
              echo "| **Artifacts** | ✅ Enabled | ${TEST_OUTPUT_ARTIFACT_RETENTION_DAYS:-7} day retention |"
            } >> performance-report.md

            # Show output strategy summary
            SUITE_COUNT=${{ steps.process-tests.outputs.suite_count || 0 }}
            if [[ $SUITE_COUNT -gt 0 ]]; then
              FULL_MODE_COUNT=0
              FAILURES_ONLY_COUNT=0

              for stats_file in test-stats-*.json; do
                if [ -f "$stats_file" ]; then
                  MODE=$(jq -r '.test_mode // "unknown"' "$stats_file")
                  if [[ "$MODE" == "FULL" ]]; then
                    FULL_MODE_COUNT=$((FULL_MODE_COUNT + 1))
                  elif [[ "$MODE" == "FAILURES_ONLY" ]]; then
                    FAILURES_ONLY_COUNT=$((FAILURES_ONLY_COUNT + 1))
                  fi
                fi
              done

              {
                echo ""
                echo "**Output Strategy Summary:**"
                echo "- $FULL_MODE_COUNT suite(s) used FULL mode (complete output)"
                echo "- $FAILURES_ONLY_COUNT suite(s) used FAILURES_ONLY mode (efficient extraction)"
              } >> performance-report.md

              if [[ $FAILURES_ONLY_COUNT -gt 0 ]]; then
                echo "- Estimated output size reduction: ~80-90% for large test suites" >> performance-report.md
              fi
            fi

            # Process Lines of Code Summary
            DISPLAYED_LOC_SUMMARY=false
            for stats_file in test-stats-*.json; do
              if [ -f "$stats_file" ] && [ "$DISPLAYED_LOC_SUMMARY" = false ]; then
                TEST_FILES_COUNT=$(jq -r '.loc_test_files' "$stats_file")
                GO_FILES_COUNT=$(jq -r '.loc_go_files' "$stats_file")
                TOTAL_LOC=$(jq -r '.loc_total' "$stats_file")
                LOC_DATE=$(jq -r '.loc_date' "$stats_file")

                if [[ "$TEST_FILES_COUNT" != "null" ]] && [[ "$GO_FILES_COUNT" != "null" ]] && [[ "$TOTAL_LOC" != "null" ]]; then
                  {
                    echo ""
                    echo ""
                    echo "### 📊 Lines of Code Summary"
                    echo "| Type | Total Lines | Date |"
                    echo "|------|-------------|------|"
                    echo "| Test Files | $TEST_FILES_COUNT | $LOC_DATE |"
                    echo "| Go Files | $GO_FILES_COUNT | $LOC_DATE |"
                    echo ""
                    echo "**Total lines of code: $TOTAL_LOC**"
                  } >> performance-report.md
                  DISPLAYED_LOC_SUMMARY=true
                fi
              fi
            done
          fi

      # ————————————————————————————————————————————————————————————————
      # Process fuzz test statistics
      # ————————————————————————————————————————————————————————————————
      - name: 🎯 Process Fuzz Test Statistics
        id: process-fuzz
        run: |
          # Process fuzz test statistics - always show status
          {
            echo ""
            echo ""
            echo "### 🎯 Fuzz Test Performance"
          } >> performance-report.md

          # Check if fuzz testing is enabled in environment
          if [[ "${{ env.ENABLE_FUZZ_TESTING }}" == "true" ]]; then
            # Fuzz testing is enabled, check for stats files
            if compgen -G "fuzz-stats-*.json" >/dev/null 2>&1; then
              {
                echo "| Fuzz Suite | Duration | Fuzz Tests | Status | Enabled |"
                echo "|------------|----------|------------|--------|---------|"
              } >> performance-report.md

              for stats_file in fuzz-stats-*.json; do
                if [ -f "$stats_file" ]; then
                  NAME=$(jq -r '.name' "$stats_file")
                  DURATION=$(jq -r '.duration_seconds' "$stats_file")
                  FUZZ_TEST_COUNT=$(jq -r '.fuzz_test_count' "$stats_file")
                  STATUS=$(jq -r '.status' "$stats_file")

                  DURATION_MIN=$((DURATION / 60))
                  DURATION_SEC=$((DURATION % 60))

                  STATUS_ICON=$([[ "$STATUS" == "success" ]] && echo "✅" || echo "❌")

                  echo "| $NAME | ${DURATION_MIN}m ${DURATION_SEC}s | $FUZZ_TEST_COUNT | $STATUS_ICON | 🎯 |" >> performance-report.md
                fi
              done
            else
              # Fuzz testing enabled but no stats found
              {
                echo "| Status | Details |"
                echo "|--------|---------|"
                echo "| **Fuzz Testing** | ✅ Enabled |"
                echo "| **Execution** | ⚠️ No fuzz stats found - check job logs |"
                echo "| **Platform** | Linux with primary Go version |"
              } >> performance-report.md
            fi
          else
            # Fuzz testing is disabled
            {
              echo "| Status | Details |"
              echo "|--------|---------|"
              echo "| **Fuzz Testing** | ❌ Disabled |"
              echo "| **Configuration** | Set ENABLE_FUZZ_TESTING=true to enable |"
              echo "| **Target Platform** | Would run on Linux with primary Go version |"
            } >> performance-report.md
          fi

      # ————————————————————————————————————————————————————————————————
      # Process coverage statistics
      # ————————————————————————————————————————————————————————————————
      - name: 📊 Process Coverage Statistics
        id: process-coverage
        run: |
          # Process coverage statistics if available
          if compgen -G "coverage-stats-*.json" >/dev/null 2>&1; then
            {
              echo ""
              echo ""
              echo "### 📊 Coverage System Performance"
            } >> performance-report.md

            for stats_file in coverage-stats-*.json; do
              if [ -f "$stats_file" ]; then
                COVERAGE_PERCENT=$(jq -r '.coverage_percent // "N/A"' "$stats_file")
                PROCESSING_TIME=$(jq -r '.processing_time_seconds // "N/A"' "$stats_file")
                FILES_PROCESSED=$(jq -r '.files_processed // "N/A"' "$stats_file")
                BADGE_GENERATED=$(jq -r '.badge_generated // "false"' "$stats_file")
                PAGES_DEPLOYED=$(jq -r '.pages_deployed // "false"' "$stats_file")

                {
                  echo "| Metric | Value |"
                  echo "|--------|-------|"
                  echo "| **Coverage Percentage** | $COVERAGE_PERCENT% |"
                  echo "| **Processing Time** | ${PROCESSING_TIME}s |"
                  echo "| **Files Processed** | $FILES_PROCESSED |"
                  echo "| **Badge Generated** | $([ "$BADGE_GENERATED" == "true" ] && echo "✅ Yes" || echo "❌ No") |"
                  echo "| **Pages Deployed** | $([ "$PAGES_DEPLOYED" == "true" ] && echo "✅ Yes" || echo "❌ No") |"
                } >> performance-report.md

                break # Only show first coverage stats file
              fi
            done
          elif [[ "${{ env.ENABLE_CODE_COVERAGE }}" == "true" ]]; then
            {
              echo ""
              echo ""
              echo "### 📊 Coverage System Status"
              echo "| Status | Details |"
              echo "|--------|---------|"
              echo "| **System** | Internal GoFortress Coverage |"
              echo "| **Threshold** | ${{ env.GO_COVERAGE_THRESHOLD }}% minimum |"
              echo "| **Badge Style** | ${{ env.GO_COVERAGE_BADGE_STYLE }} |"
              echo "| **PR Comments** | $([ "${{ env.GO_COVERAGE_POST_COMMENTS }}" == "true" ] && echo "✅ Enabled" || echo "❌ Disabled") |"
              echo "| **Theme** | ${{ env.GO_COVERAGE_REPORT_THEME }} |"
            } >> performance-report.md
          fi

      # ————————————————————————————————————————————————————————————————
      # Generate job results summary
      # ————————————————————————————————————————————————————————————————
      - name: 🔧 Generate Job Results Summary
        id: job-results
        run: |
          {
            echo ""
            echo "### 🔧 Job Results Summary"
            echo "| Job | Status | Result |"
            echo "|-----|--------|--------|"
            echo "| 🎯 Setup Configuration | ${{ inputs.setup-result }} | $([ "${{ inputs.setup-result }}" = "success" ] && echo "✅" || echo "❌") |"
            echo "| 🪄 Test MAGE-X | ${{ inputs.test-magex-result }} | $([ "${{ inputs.test-magex-result }}" = "success" ] && echo "✅" || echo "❌") |"
            echo "| 🪝 Pre-commit Checks | ${{ inputs.pre-commit-result }} | $([ "${{ inputs.pre-commit-result }}" = "success" ] && echo "✅" || echo "❌") |"
            echo "| 🔒 Security Scans | ${{ inputs.security-result }} | $([ "${{ inputs.security-result }}" = "success" ] && echo "✅" || echo "❌") |"
            echo "| 📊 Code Quality | ${{ inputs.code-quality-result }} | $([ "${{ inputs.code-quality-result }}" = "success" ] && echo "✅" || echo "❌") |"
            echo "| 🧪 Test Suite | ${{ inputs.test-suite-result }} | $([ "${{ inputs.test-suite-result }}" = "success" ] && echo "✅" || echo "❌") |"
          } >> performance-report.md

          # Only show benchmarks row if it was attempted
          if [[ "${{ inputs.benchmarks-result }}" != "skipped" ]]; then
            echo "| 🏃 Benchmarks | ${{ inputs.benchmarks-result }} | $([ "${{ inputs.benchmarks-result }}" = "success" ] && echo "✅" || echo "❌") |" >> performance-report.md
          fi

          # Always show status-check result
          echo "| 🎯 All Tests Passed | ${{ inputs.status-check-result }} | $([ "${{ inputs.status-check-result }}" = "success" ] && echo "✅" || echo "❌") |" >> performance-report.md

          # Only show release row if it was attempted
          if [[ "${{ inputs.release-result }}" != "skipped" ]]; then
            echo "| 🚀 Release | ${{ inputs.release-result }} | $([ "${{ inputs.release-result }}" = "success" ] && echo "✅" || echo "❌") |" >> performance-report.md
          fi

          echo "" >> performance-report.md

          # Add release-specific information if this was a tag push
          if [[ "${{ github.ref }}" == refs/tags/v* ]]; then
            {
              echo ""
              echo "## 📦 Release Information"
            } >> performance-report.md

            if [[ "${{ inputs.release-result }}" == "success" ]]; then
              {
                echo "✅ Release ${{ github.ref_name }} created successfully!"
                echo "[View Release](https://github.com/${{ github.repository }}/releases/tag/${{ github.ref_name }})"
              } >> performance-report.md
            elif [[ "${{ inputs.release-result }}" == "skipped" ]]; then
              echo "⏭️ Release was skipped (likely due to test failures)" >> performance-report.md
            elif [[ "${{ inputs.release-result }}" == "failure" ]]; then
              echo "❌ Release creation failed - check logs for details" >> performance-report.md
            fi
            echo "" >> performance-report.md
          fi

      # ————————————————————————————————————————————————————————————————
      # Generate performance insights
      # ————————————————————————————————————————————————————————————————
      - name: 🚀 Generate Performance Insights
        id: performance-insights
        run: |
          TOTAL_DURATION=${{ steps.calculate-timing.outputs.total_duration }}
          TOTAL_MINUTES=${{ steps.calculate-timing.outputs.total_minutes }}
          TOTAL_SECONDS=${{ steps.calculate-timing.outputs.total_seconds }}

          {
            echo ""
            echo "### 🚀 Performance Insights"
          } >> performance-report.md

          # Overall timing insights
          if [[ $TOTAL_DURATION -gt 600 ]]; then
            echo "- ⚠️  **Warning**: Workflow took longer than 10 minutes (${TOTAL_MINUTES}m ${TOTAL_SECONDS}s)" >> performance-report.md
          elif [[ $TOTAL_DURATION -gt 300 && $TOTAL_DURATION -le 600 ]]; then
            echo "- ℹ️  Workflow completed in ${TOTAL_MINUTES}m ${TOTAL_SECONDS}s." >> performance-report.md
          elif [[ $TOTAL_DURATION -gt 180 && $TOTAL_DURATION -le 300 ]]; then
            echo "- 🎉  **Great Performance**: Workflow completed in under 5 minutes (${TOTAL_MINUTES}m ${TOTAL_SECONDS}s)!" >> performance-report.md
          elif [[ $TOTAL_DURATION -le 180 ]]; then
            echo "- 🚀  **Excellent Performance**: Workflow completed in under 3 minutes!" >> performance-report.md
          fi

          # Test-specific insights based on collected data
          if compgen -G "test-stats-*.json" >/dev/null 2>&1; then
            TOTAL_TESTS=${{ steps.process-tests.outputs.total_tests || 0 }}
            TOTAL_FAILURES=${{ steps.process-tests.outputs.total_failures || 0 }}

            # Calculate test-specific metrics
            FULL_MODE_SUITES=0
            FAILURES_ONLY_SUITES=0
            ESTIMATED_SAVINGS_MB=0

            for stats_file in test-stats-*.json; do
              if [ -f "$stats_file" ]; then
                MODE=$(jq -r '.test_mode // "unknown"' "$stats_file")
                OUTPUT_SIZE=$(jq -r '.output_size_bytes // 0' "$stats_file")

                if [[ "$MODE" == "FULL" ]]; then
                  FULL_MODE_SUITES=$((FULL_MODE_SUITES + 1))
                elif [[ "$MODE" == "FAILURES_ONLY" ]]; then
                  FAILURES_ONLY_SUITES=$((FAILURES_ONLY_SUITES + 1))
                  # Estimate savings (assume FULL mode would be 5-10x larger)
                  ESTIMATED_FULL_SIZE=$((OUTPUT_SIZE * 7))
                  SAVINGS=$((ESTIMATED_FULL_SIZE - OUTPUT_SIZE))
                  ESTIMATED_SAVINGS_MB=$((ESTIMATED_SAVINGS_MB + SAVINGS / 1048576))
                fi
              fi
            done

            echo "- **Test Strategy**: $FULL_MODE_SUITES suite(s) used FULL output, $FAILURES_ONLY_SUITES used efficient FAILURES_ONLY" >> performance-report.md

            if [[ $TOTAL_FAILURES -gt 0 ]] && [[ $TOTAL_TESTS -gt 0 ]]; then
              FAILURE_RATE=$(( (TOTAL_FAILURES * 10000) / TOTAL_TESTS ))  # Calculate percentage * 100 for precision
              FAILURE_RATE_DISPLAY=$(( FAILURE_RATE / 100 )).$(( FAILURE_RATE % 100 ))
              echo "- **Failure Rate**: $FAILURE_RATE_DISPLAY% ($TOTAL_FAILURES/$TOTAL_TESTS tests)" >> performance-report.md

              # Find most affected package
              MOST_AFFECTED_PACKAGE=""
              MAX_FAILURES=0
              for stats_file in test-stats-*.json; do
                if [ -f "$stats_file" ]; then
                  FAILURE_DETAILS=$(jq -r '.failure_details // null' "$stats_file")
                  if [[ "$FAILURE_DETAILS" != "null" ]] && [[ "$FAILURE_DETAILS" != "[]" ]]; then
                    # Extract package with most failures
                    PACKAGE_FAILURES=$(echo "$FAILURE_DETAILS" | jq -r 'group_by(.Package) | map({package: .[0].Package, count: length}) | max_by(.count)' 2>/dev/null)
                    if [[ "$PACKAGE_FAILURES" != "null" ]]; then
                      PKG_NAME=$(echo "$PACKAGE_FAILURES" | jq -r '.package | split("/") | .[-1] // .[-2] // .')
                      PKG_COUNT=$(echo "$PACKAGE_FAILURES" | jq -r '.count')
                      if [[ $PKG_COUNT -gt $MAX_FAILURES ]]; then
                        MAX_FAILURES=$PKG_COUNT
                        MOST_AFFECTED_PACKAGE="$PKG_NAME"
                      fi
                    fi
                  fi
                fi
              done

              if [[ -n "$MOST_AFFECTED_PACKAGE" ]] && [[ $MAX_FAILURES -gt 0 ]]; then
                echo "- **Most Affected Package**: $MOST_AFFECTED_PACKAGE ($MAX_FAILURES failures)" >> performance-report.md
              fi
            elif [[ $TOTAL_TESTS -gt 0 ]]; then
              echo "- **Test Success**: All $TOTAL_TESTS tests passed! 🎉" >> performance-report.md
            fi

            if [[ $ESTIMATED_SAVINGS_MB -gt 0 ]]; then
              echo "- **Output Efficiency**: Saved approximately ${ESTIMATED_SAVINGS_MB}MB by using FAILURES_ONLY mode" >> performance-report.md
            fi
          fi

          # Standard insights
          {
            echo "- **Parallel Jobs**: Multiple jobs ran in parallel to optimize execution time"
            echo "- **Matrix Strategy**: Tests ran across $(echo '${{ inputs.test-matrix }}' | jq '.include | length') configurations"
          } >> performance-report.md

          if [ "${{ env.ENABLE_VERBOSE_TEST_OUTPUT }}" != "true" ]; then
            echo "- **Verbose Output**: Disabled to speed up test execution" >> performance-report.md
          else
            echo "- **Verbose Output**: Enabled for detailed test logs" >> performance-report.md
          fi

          # Add failure analysis if any job failed
          FAILED_JOBS=""
          [ "${{ inputs.setup-result }}" != "success" ] && [ "${{ inputs.setup-result }}" != "skipped" ] && FAILED_JOBS="${FAILED_JOBS}Setup Configuration, "
          [ "${{ inputs.test-magex-result }}" != "success" ] && [ "${{ inputs.test-magex-result }}" != "skipped" ] && FAILED_JOBS="${FAILED_JOBS}Test MAGE-X, "
          [ "${{ inputs.pre-commit-result }}" != "success" ] && [ "${{ inputs.pre-commit-result }}" != "skipped" ] && FAILED_JOBS="${FAILED_JOBS}Pre-commit Checks, "
          [ "${{ inputs.security-result }}" != "success" ] && [ "${{ inputs.security-result }}" != "skipped" ] && FAILED_JOBS="${FAILED_JOBS}Security Scans, "
          [ "${{ inputs.code-quality-result }}" != "success" ] && [ "${{ inputs.code-quality-result }}" != "skipped" ] && FAILED_JOBS="${FAILED_JOBS}Code Quality, "
          [ "${{ inputs.test-suite-result }}" != "success" ] && [ "${{ inputs.test-suite-result }}" != "skipped" ] && FAILED_JOBS="${FAILED_JOBS}Test Suite, "
          [ "${{ inputs.benchmarks-result }}" != "success" ] && [ "${{ inputs.benchmarks-result }}" != "skipped" ] && FAILED_JOBS="${FAILED_JOBS}Benchmarks, "
          [ "${{ inputs.status-check-result }}" != "success" ] && [ "${{ inputs.status-check-result }}" != "skipped" ] && FAILED_JOBS="${FAILED_JOBS}Status Check, "
          [ "${{ inputs.release-result }}" != "success" ] && [ "${{ inputs.release-result }}" != "skipped" ] && FAILED_JOBS="${FAILED_JOBS}Release, "

          if [ -n "$FAILED_JOBS" ]; then
            FAILED_JOBS=${FAILED_JOBS%, }  # Remove trailing comma
            {
              echo ""
              echo ""
              echo "### ❌ Failed Jobs"
              echo "The following jobs did not complete successfully:"
              echo "- ${FAILED_JOBS}"
            } >> performance-report.md
          fi

      # ————————————————————————————————————————————————————————————————
      # Finalize performance report
      # ————————————————————————————————————————————————————————————————
      - name: ✅ Finalize Performance Report
        id: finalize-report
        run: |
          # Write the final report to GitHub Step Summary
          cat performance-report.md >> $GITHUB_STEP_SUMMARY
          echo "✅ Performance summary report generated successfully"
