# ------------------------------------------------------------------------------------
#  Workflow Completion Report (Reusable Workflow) (GoFortress)
#
#  Purpose: Generate a comprehensive workflow completion report for the entire
#  workflow run, including timing metrics, test results, job status, and analytics.
#
#  Maintainer: @mrz1836
#
# ------------------------------------------------------------------------------------

name: GoFortress (Completion Report)

on:
  workflow_call:
    inputs:
      benchmarks-result:
        description: "Benchmarks job result"
        required: false
        type: string
        default: "skipped"
      start-epoch:
        description: "Workflow start epoch time"
        required: true
        type: string
      start-time:
        description: "Workflow start time"
        required: true
        type: string
      setup-result:
        description: "Setup job result"
        required: true
        type: string
      test-magex-result:
        description: "Test MAGE-X job result"
        required: true
        type: string
      security-result:
        description: "Security job result"
        required: true
        type: string
      code-quality-result:
        description: "Code quality job result"
        required: true
        type: string
      pre-commit-result:
        description: "Pre-commit checks job result"
        required: true
        type: string
      test-suite-result:
        description: "Test suite job result"
        required: true
        type: string
      release-result:
        description: "Result of the release job"
        required: false
        type: string
        default: "skipped"
      status-check-result:
        description: "Result of the status-check job"
        required: false
        type: string
        default: "skipped"
      test-matrix:
        description: "Test matrix JSON"
        required: true
        type: string
      env-json:
        description: "JSON string of environment variables"
        required: true
        type: string
      primary-runner:
        description: "Primary runner OS"
        required: true
        type: string

# Security: Restrictive default permissions with job-level overrides for least privilege access
permissions:
  contents: read

jobs:
  # ----------------------------------------------------------------------------------
  # Workflow Completion Report
  # ----------------------------------------------------------------------------------
  completion-report:
    name: 📊 Final Workflow Report
    runs-on: ${{ inputs.primary-runner }}
    steps:
      # ————————————————————————————————————————————————————————————————
      # Parse environment variables
      # ————————————————————————————————————————————————————————————————
      - name: 🔧 Parse environment variables
        env:
          ENV_JSON: ${{ inputs.env-json }}
        run: |
          echo "📋 Setting environment variables..."
          echo "$ENV_JSON" | jq -r 'to_entries | .[] | "\(.key)=\(.value)"' | while IFS='=' read -r key value; do
            echo "$key=$value" >> $GITHUB_ENV
          done

      # ————————————————————————————————————————————————————————————————
      # Download all statistics artifacts
      # ————————————————————————————————————————————————————————————————
      - name: 📥 Download performance artifacts
        if: always()
        uses: actions/download-artifact@634f93cb2916e3fdff6788551b99b062d0335ce0 # v5.0.0
        with:
          pattern: "*-stats-*"
          path: ./performance-artifacts/

      - name: 📥 Download test result artifacts for failure analysis
        if: always()
        uses: actions/download-artifact@634f93cb2916e3fdff6788551b99b062d0335ce0 # v5.0.0
        continue-on-error: true
        with:
          pattern: "test-results-*"
          path: ./test-artifacts/

      # ————————————————————————————————————————————————————————————————
      # Flatten performance artifacts for processing
      # ————————————————————————————————————————————————————————————————
      - name: 🗂️ Flatten performance and test artifacts
        if: always()
        run: |
          echo "🗂️ Flattening downloaded artifacts..."

          # Find all JSON files in subdirectories and move them to current directory
          if [ -d "./performance-artifacts/" ]; then
            find ./performance-artifacts/ -name "*.json" -type f | while read -r file; do
              filename=$(basename "$file")
              echo "Moving $file to ./$filename"
              cp "$file" "./$filename"
            done

            # List all flattened files for debugging
            echo "📋 Available stats files:"
            ls -la *-stats-*.json 2>/dev/null || echo "No stats files found"
          else
            echo "⚠️ No performance-artifacts directory found"
          fi

          # Process test artifacts for additional failure details
          if [ -d "./test-artifacts/" ]; then
            find ./test-artifacts/ -name "*.json" -type f | while read -r file; do
              filename=$(basename "$file")
              echo "Moving test artifact $file to ./$filename"
              cp "$file" "./$filename"
            done

            echo "📋 Available test artifact files:"
            ls -la test-failures*.json 2>/dev/null || echo "No test failure files found"
          else
            echo "⚠️ No test-artifacts directory found"
          fi

      # ————————————————————————————————————————————————————————————————
      # Calculate timing metrics
      # ————————————————————————————————————————————————————————————————
      - name: ⏱️ Calculate Timing Metrics
        id: calculate-timing
        run: |
          # Calculate total duration
          START_EPOCH=${{ inputs.start-epoch }}
          END_EPOCH=$(date +%s)
          TOTAL_DURATION=$((END_EPOCH - START_EPOCH))
          TOTAL_MINUTES=$((TOTAL_DURATION / 60))
          TOTAL_SECONDS=$((TOTAL_DURATION % 60))

          # Store as outputs for later use
          echo "total_minutes=$TOTAL_MINUTES" >> $GITHUB_OUTPUT
          echo "total_seconds=$TOTAL_SECONDS" >> $GITHUB_OUTPUT
          echo "total_duration=$TOTAL_DURATION" >> $GITHUB_OUTPUT

      # ————————————————————————————————————————————————————————————————
      # Initialize completion report
      # ————————————————————————————————————————————————————————————————
      - name: 📝 Initialize Completion Report
        id: init-report
        run: |
          # Create the initial completion report with professional structure
          SUMMARY_TIME=$(date -u +"%Y-%m-%d %H:%M:%S UTC")

          {
            echo "# 🏰 GoFortress Workflow Completion Report"
            echo "_Generated at: ${SUMMARY_TIME}_"
            echo ""
            echo ""
            echo "## 🏁 Workflow Summary"
            echo ""
            echo "### ⏱️ Execution Timeline"
            echo "| Metric | Value |"
            echo "|--------|-------|"
            echo "| **Total Duration** | ${{ steps.calculate-timing.outputs.total_minutes }}m ${{ steps.calculate-timing.outputs.total_seconds }}s |"
            echo "| **Start Time** | ${{ inputs.start-time }} |"
            echo "| **End Time** | $(date -u +"%Y-%m-%dT%H:%M:%SZ") |"
            echo "| **Workflow** | ${{ github.workflow }} |"
            echo "| **Run Number** | ${{ github.run_number }} |"
            echo "| **Trigger** | ${{ github.event_name }} |"
            echo "| **Source** | ${{ github.event.pull_request.head.repo.full_name == github.repository && 'Internal' || 'Fork' }} |"
            echo ""
            echo "<br><br>"
          } > completion-report.md

      # ————————————————————————————————————————————————————————————————
      # Process cache statistics
      # ————————————————————————————————————————————————————————————————
      - name: 💾 Process Cache Statistics
        id: process-cache
        run: |
          # Process cache statistics if available
          if compgen -G "cache-stats-*.json" >/dev/null 2>&1; then
            {
              echo ""
              echo "### 💾 Cache Statistics"
              echo "| Workflow/Job | OS | Go Version | Module Cache | Build Cache | Module Size | Build Size |"
              echo "|--------------|----|-----------|--------------|-----------|-----------|------------|"
            } >> completion-report.md

            TOTAL_CACHE_HITS=0
            TOTAL_CACHE_ATTEMPTS=0
            WORKFLOWS_WITH_CACHE=""

            for stats_file in cache-stats-*.json; do
              if [ -f "$stats_file" ]; then
                OS=$(jq -r '.os' "$stats_file")
                GO_VER=$(jq -r '.go_version' "$stats_file")
                WORKFLOW=$(jq -r '.workflow // "unknown"' "$stats_file")
                JOB_NAME=$(jq -r '.job_name // ""' "$stats_file")
                GOMOD_HIT=$(jq -r '.gomod_cache_hit' "$stats_file")
                GOBUILD_HIT=$(jq -r '.gobuild_cache_hit' "$stats_file")
                GOMOD_SIZE=$(jq -r '.cache_size_gomod' "$stats_file")
                GOBUILD_SIZE=$(jq -r '.cache_size_gobuild' "$stats_file")

                GOMOD_ICON=$([[ "$GOMOD_HIT" == "true" ]] && echo "✅ Hit" || echo "❌ Miss")
                GOBUILD_ICON=$([[ "$GOBUILD_HIT" == "true" ]] && echo "✅ Hit" || echo "❌ Miss")

                # Create workflow/job identifier
                if [[ -n "$JOB_NAME" && "$JOB_NAME" != "null" ]]; then
                  WORKFLOW_JOB="${WORKFLOW}/${JOB_NAME}"
                else
                  WORKFLOW_JOB="${WORKFLOW}"
                fi

                echo "| $WORKFLOW_JOB | $OS | $GO_VER | $GOMOD_ICON | $GOBUILD_ICON | $GOMOD_SIZE | $GOBUILD_SIZE |" >> completion-report.md

                [[ "$GOMOD_HIT" == "true" ]] && TOTAL_CACHE_HITS=$((TOTAL_CACHE_HITS + 1))
                [[ "$GOBUILD_HIT" == "true" ]] && TOTAL_CACHE_HITS=$((TOTAL_CACHE_HITS + 1))
                TOTAL_CACHE_ATTEMPTS=$((TOTAL_CACHE_ATTEMPTS + 2))

                # Track workflows that used cache
                if [[ "$WORKFLOWS_WITH_CACHE" != *"$WORKFLOW"* ]]; then
                  if [[ -z "$WORKFLOWS_WITH_CACHE" ]]; then
                    WORKFLOWS_WITH_CACHE="$WORKFLOW"
                  else
                    WORKFLOWS_WITH_CACHE="${WORKFLOWS_WITH_CACHE}, $WORKFLOW"
                  fi
                fi
              fi
            done

            # Add cache efficiency summary
            if [[ $TOTAL_CACHE_ATTEMPTS -gt 0 ]]; then
              CACHE_HIT_RATE=$((TOTAL_CACHE_HITS * 100 / TOTAL_CACHE_ATTEMPTS))
              {
                echo ""
                echo "**Cache Performance Summary:**"
                echo "- **Overall Hit Rate**: ${CACHE_HIT_RATE}% (${TOTAL_CACHE_HITS}/${TOTAL_CACHE_ATTEMPTS} cache operations)"
                echo "- **Workflows Using Cache**: $WORKFLOWS_WITH_CACHE"
              } >> completion-report.md

              if [[ $CACHE_HIT_RATE -ge 80 ]]; then
                echo "- **Cache Efficiency**: 🚀 Excellent (${CACHE_HIT_RATE}% hit rate)" >> completion-report.md
              elif [[ $CACHE_HIT_RATE -ge 60 ]]; then
                echo "- **Cache Efficiency**: ✅ Good (${CACHE_HIT_RATE}% hit rate)" >> completion-report.md
              elif [[ $CACHE_HIT_RATE -ge 40 ]]; then
                echo "- **Cache Efficiency**: ⚠️ Fair (${CACHE_HIT_RATE}% hit rate)" >> completion-report.md
              else
                echo "- **Cache Efficiency**: ❌ Poor (${CACHE_HIT_RATE}% hit rate - consider optimizing cache strategy)" >> completion-report.md
              fi
            fi
          fi

          # Add spacing after cache section
          if compgen -G "cache-stats-*.json" >/dev/null 2>&1; then
            echo "" >> completion-report.md
            echo "<br><br>" >> completion-report.md
          fi

      # ————————————————————————————————————————————————————————————————
      # Process benchmark statistics
      # ————————————————————————————————————————————————————————————————
      - name: 🏃 Process Benchmark Statistics
        id: process-benchmarks
        run: |
          # Process benchmark statistics if available
          if compgen -G "benchmark-stats-*.json" >/dev/null 2>&1; then
            {
              echo ""
              echo ""
              echo "### ⚡ Benchmark Results"
            } >> completion-report.md

            # Get benchmark mode from the first stats file
            BENCH_MODE="normal"
            for stats_file in benchmark-stats-*.json; do
              if [ -f "$stats_file" ]; then
                BENCH_MODE=$(jq -r '.benchmark_mode // "normal"' "$stats_file")
                break
              fi
            done

            {
              echo ""
              echo "**Mode**: \`$BENCH_MODE\` $(case "$BENCH_MODE" in quick) echo "(Quick 50ms runs)" ;; full) echo "(Comprehensive 10s runs)" ;; *) echo "(Normal 100ms runs)" ;; esac)"
              echo ""
              echo "| Benchmark Suite | Duration | Benchmarks | Status |"
              echo "|-----------------|----------|------------|--------|"
            } >> completion-report.md

            for stats_file in benchmark-stats-*.json; do
              if [ -f "$stats_file" ]; then
                NAME=$(jq -r '.name' "$stats_file")
                DURATION=$(jq -r '.duration_seconds' "$stats_file")
                BENCHMARK_COUNT=$(jq -r '.benchmark_count' "$stats_file")
                STATUS=$(jq -r '.status' "$stats_file")

                DURATION_MIN=$((DURATION / 60))
                DURATION_SEC=$((DURATION % 60))
                STATUS_ICON=$([[ "$STATUS" == "success" ]] && echo "✅" || echo "❌")

                echo "| $NAME | ${DURATION_MIN}m ${DURATION_SEC}s | $BENCHMARK_COUNT | $STATUS_ICON |" >> completion-report.md
              fi
            done

            # Display detailed benchmark results
            {
              echo ""
              echo "<details>"
              echo "<summary>Detailed Benchmark Results</summary>"
              echo ""
            } >> completion-report.md

            for stats_file in benchmark-stats-*.json; do
              if [ -f "$stats_file" ]; then
                NAME=$(jq -r '.name' "$stats_file")
                BENCHMARK_SUMMARY=$(jq -r '.benchmark_summary' "$stats_file")
                if [ -n "$BENCHMARK_SUMMARY" ] && [ "$BENCHMARK_SUMMARY" != "null" ]; then
                  {
                    echo "#### $NAME"
                    echo "$BENCHMARK_SUMMARY"
                    echo ""
                  } >> completion-report.md
                fi
              fi
            done

            echo "</details><br><br>" >> completion-report.md
          fi

      # ————————————————————————————————————————————————————————————————
      # Process test statistics
      # ————————————————————————————————————————————————————————————————
      - name: 🧪 Process Test Statistics
        id: process-tests
        run: |
          # Process test statistics if available
          if compgen -G "test-stats-*.json" >/dev/null 2>&1; then
            {
              echo ""
              echo ""
              echo "### 🧪 Test Results Summary"
              echo "| Test Suite | Mode | Duration | Tests | Failed | Packages | Status | Race | Coverage |"
              echo "|------------|------|----------|-------|--------|----------|--------|------|----------|"
            } >> completion-report.md

            # Initialize totals for summary
            TOTAL_TESTS=0
            TOTAL_FAILURES=0
            TOTAL_AFFECTED_PACKAGES=0
            SUITE_COUNT=0

            for stats_file in test-stats-*.json; do
              if [ -f "$stats_file" ]; then
                NAME=$(jq -r '.name' "$stats_file")
                DURATION=$(jq -r '.duration_seconds' "$stats_file")
                TEST_COUNT=$(jq -r '.test_count' "$stats_file")
                STATUS=$(jq -r '.status' "$stats_file")
                RACE_ENABLED=$(jq -r '.race_enabled' "$stats_file")
                COVERAGE_ENABLED=$(jq -r '.coverage_enabled' "$stats_file")

                # New enhanced fields
                TEST_MODE=$(jq -r '.test_mode // "unknown"' "$stats_file")
                SUITE_FAILURES=$(jq -r '.total_failures // 0' "$stats_file")
                AFFECTED_PACKAGES=$(jq -r '.affected_packages // 0' "$stats_file")

                DURATION_MIN=$((DURATION / 60))
                DURATION_SEC=$((DURATION % 60))

                COVERAGE_ICON=$([[ "$COVERAGE_ENABLED" == "true" ]] && echo "✅" || echo "❌")
                RACE_ICON=$([[ "$RACE_ENABLED" == "true" ]] && echo "✅" || echo "❌")
                STATUS_ICON=$([[ "$STATUS" == "success" ]] && echo "✅" || echo "❌")

                # Show package count or dash
                PACKAGE_DISPLAY=$([[ "$AFFECTED_PACKAGES" -gt 0 ]] && echo "$AFFECTED_PACKAGES" || echo "-")

                echo "| $NAME | $TEST_MODE | ${DURATION_MIN}m ${DURATION_SEC}s | $TEST_COUNT | $SUITE_FAILURES | $PACKAGE_DISPLAY | $STATUS_ICON | $RACE_ICON | $COVERAGE_ICON |" >> completion-report.md

                # Accumulate totals
                TOTAL_TESTS=$((TOTAL_TESTS + TEST_COUNT))
                TOTAL_FAILURES=$((TOTAL_FAILURES + SUITE_FAILURES))
                TOTAL_AFFECTED_PACKAGES=$((TOTAL_AFFECTED_PACKAGES + AFFECTED_PACKAGES))
                SUITE_COUNT=$((SUITE_COUNT + 1))
              fi
            done

            # Store totals as outputs for later use
            echo "total_tests=$TOTAL_TESTS" >> $GITHUB_OUTPUT
            echo "total_failures=$TOTAL_FAILURES" >> $GITHUB_OUTPUT
            echo "suite_count=$SUITE_COUNT" >> $GITHUB_OUTPUT

            # Add test failure analysis if any failures exist
            if [[ $TOTAL_FAILURES -gt 0 ]]; then
              {
                echo ""
                echo ""
                echo "### ❌ Test Failure Analysis"
                echo "**Total Failures**: $TOTAL_FAILURES across $SUITE_COUNT test suite(s)"
                echo ""
                echo "#### 📊 Failures by Test Suite:"
              } >> completion-report.md

              for stats_file in test-stats-*.json; do
                if [ -f "$stats_file" ]; then
                  SUITE_NAME=$(jq -r '.name' "$stats_file")
                  SUITE_FAILURES=$(jq -r '.total_failures // 0' "$stats_file")
                  SUITE_PACKAGES=$(jq -r '.affected_packages // 0' "$stats_file")

                  if [[ $SUITE_FAILURES -gt 0 ]]; then
                    echo "- **$SUITE_NAME**: $SUITE_FAILURES failures across $SUITE_PACKAGES packages" >> completion-report.md
                  fi
                fi
              done

              {
                echo ""
                echo "<details>"
                echo "<summary>🔍 Top Failed Tests (click to expand)</summary>"
                echo ""
                echo "| Test Name | Package | Duration | Suite |"
                echo "|-----------|---------|----------|-------|"
              } >> completion-report.md

              # Extract detailed failure information from all suites
              FAILURE_COUNT=0
              for stats_file in test-stats-*.json; do
                if [ -f "$stats_file" ] && [[ $FAILURE_COUNT -lt 20 ]]; then
                  SUITE_NAME=$(jq -r '.name' "$stats_file")
                  FAILURE_DETAILS=$(jq -r '.failure_details // null' "$stats_file")

                  if [[ "$FAILURE_DETAILS" != "null" ]] && [[ "$FAILURE_DETAILS" != "[]" ]]; then
                    echo "$FAILURE_DETAILS" | jq -r --arg suite "$SUITE_NAME" \
                      '.[] | "| \(.Test) | \(.Package | split("/") | .[-1] // .[-2] // .) | \(.Duration // "unknown")s | \($suite) |"' 2>/dev/null | \
                      head -10 >> completion-report.md || true
                    FAILURE_COUNT=$((FAILURE_COUNT + 10))
                  fi
                fi
              done

              {
                echo ""
                echo "</details>"
              } >> completion-report.md

              # Add error details section for failed tests
              HAS_ERROR_OUTPUT=false
              for stats_file in test-stats-*.json; do
                if [ -f "$stats_file" ]; then
                  FAILURE_DETAILS=$(jq -r '.failure_details // null' "$stats_file")
                  if [[ "$FAILURE_DETAILS" != "null" ]] && [[ "$FAILURE_DETAILS" != "[]" ]]; then
                    # Check if any failure has non-empty output
                    HAS_OUTPUT=$(echo "$FAILURE_DETAILS" | jq -r 'map(select(.Output != "" and .Output != null)) | length > 0' 2>/dev/null)
                    if [[ "$HAS_OUTPUT" == "true" ]]; then
                      HAS_ERROR_OUTPUT=true
                      break
                    fi
                  fi
                fi
              done

              if [[ "$HAS_ERROR_OUTPUT" == "true" ]]; then
                {
                  echo ""
                  echo ""
                  echo "### 📝 Test Error Messages"
                  echo ""
                } >> completion-report.md

                ERROR_COUNT=0
                for stats_file in test-stats-*.json; do
                  if [ -f "$stats_file" ] && [[ $ERROR_COUNT -lt 10 ]]; then
                    SUITE_NAME=$(jq -r '.name' "$stats_file")
                    FAILURE_DETAILS=$(jq -r '.failure_details // null' "$stats_file")

                    if [[ "$FAILURE_DETAILS" != "null" ]] && [[ "$FAILURE_DETAILS" != "[]" ]]; then
                      # Display failures with non-empty outputs using smart truncation
                      echo "$FAILURE_DETAILS" | jq -r --arg suite "$SUITE_NAME" \
                        '.[] | select(.Output != "" and .Output != null) |
                        "#### \(.Test) (\(.Package | split("/") | .[-1] // .[-2] // .))\n\n```\n\(.Output | if length > 1500 then .[0:1500] + "\n... (truncated)" else . end)\n```\n"' 2>/dev/null | \
                        head -c 4000 >> completion-report.md || true
                      ERROR_COUNT=$((ERROR_COUNT + 3))
                    fi
                  fi
                done
              fi
            fi
          fi

      # ————————————————————————————————————————————————————————————————
      # Add test configuration and LOC summary
      # ————————————————————————————————————————————————————————————————
      - name: 🎛️ Add Test Configuration Section
        id: add-test-config
        run: |
          # Add test output configuration section
          if compgen -G "test-stats-*.json" >/dev/null 2>&1; then
            {
              echo "<br><br>"
              echo ""
              echo "### 🎛️ Test Output Configuration"
              echo "| Configuration | Value | Description |"
              echo "|---------------|-------|-------------|"
              echo "| **Mode** | ${TEST_OUTPUT_MODE:-SMART} | Test output strategy |"
              echo "| **Smart Threshold** | ${TEST_OUTPUT_SMART_THRESHOLD:-500} tests | Switch to FAILURES_ONLY above this |"
              echo "| **Detail Count** | ${TEST_FAILURE_DETAIL_COUNT:-50} | Failures shown with details |"
              echo "| **Annotations** | ${TEST_FAILURE_ANNOTATION_COUNT:-10} | GitHub annotations limit |"
              echo "| **Artifacts** | ✅ Enabled | ${TEST_OUTPUT_ARTIFACT_RETENTION_DAYS:-7} day retention |"
            } >> completion-report.md

            # Show output strategy summary
            SUITE_COUNT=${{ steps.process-tests.outputs.suite_count || 0 }}
            if [[ $SUITE_COUNT -gt 0 ]]; then
              FULL_MODE_COUNT=0
              FAILURES_ONLY_COUNT=0

              for stats_file in test-stats-*.json; do
                if [ -f "$stats_file" ]; then
                  MODE=$(jq -r '.test_mode // "unknown"' "$stats_file")
                  if [[ "$MODE" == "FULL" ]]; then
                    FULL_MODE_COUNT=$((FULL_MODE_COUNT + 1))
                  elif [[ "$MODE" == "FAILURES_ONLY" ]]; then
                    FAILURES_ONLY_COUNT=$((FAILURES_ONLY_COUNT + 1))
                  fi
                fi
              done

              {
                echo ""
                echo "**Output Strategy Summary:**"
                echo "- $FULL_MODE_COUNT suite(s) used FULL mode (complete output)"
                echo "- $FAILURES_ONLY_COUNT suite(s) used FAILURES_ONLY mode (efficient extraction)"
              } >> completion-report.md

              if [[ $FAILURES_ONLY_COUNT -gt 0 ]]; then
                echo "- Estimated output size reduction: ~80-90% for large test suites" >> completion-report.md
              fi
            fi

            # LOC Summary has been moved to dedicated section - no longer needed here
          fi

      # ————————————————————————————————————————————————————————————————
      # Process fuzz test statistics
      # ————————————————————————————————————————————————————————————————
      - name: 🎯 Process Fuzz Test Statistics
        id: process-fuzz
        run: |
          # Process fuzz test statistics - always show status
          {
            echo "<br><br>"
            echo ""
            echo "### 🛡️ Security Testing Results"
          } >> completion-report.md

          # Check if fuzz testing is enabled in environment
          if [[ "${{ env.ENABLE_FUZZ_TESTING }}" == "true" ]]; then
            # Fuzz testing is enabled, check for stats files
            if compgen -G "fuzz-stats-*.json" >/dev/null 2>&1; then
              # Check if we have actual fuzz stats data before creating table header
              HAS_FUZZ_DATA=false
              for stats_file in fuzz-stats-*.json; do
                if [ -f "$stats_file" ]; then
                  NAME=$(jq -r '.name' "$stats_file")
                  if [[ "$NAME" != "null" ]] && [[ -n "$NAME" ]]; then
                    HAS_FUZZ_DATA=true
                    break
                  fi
                fi
              done

              if [[ "$HAS_FUZZ_DATA" == "true" ]]; then
                # Create table header only when we have actual data
                {
                  echo "| Fuzz Suite | Duration | Fuzz Tests | Status | Enabled |"
                  echo "|------------|----------|------------|--------|---------|"
                } >> completion-report.md

                for stats_file in fuzz-stats-*.json; do
                  if [ -f "$stats_file" ]; then
                    NAME=$(jq -r '.name' "$stats_file")
                    DURATION=$(jq -r '.duration_seconds' "$stats_file")
                    FUZZ_TEST_COUNT=$(jq -r '.fuzz_test_count' "$stats_file")
                    STATUS=$(jq -r '.status' "$stats_file")

                    if [[ "$NAME" != "null" ]] && [[ -n "$NAME" ]]; then
                      DURATION_MIN=$((DURATION / 60))
                      DURATION_SEC=$((DURATION % 60))

                      STATUS_ICON=$([[ "$STATUS" == "success" ]] && echo "✅" || echo "❌")

                      echo "| $NAME | ${DURATION_MIN}m ${DURATION_SEC}s | $FUZZ_TEST_COUNT | $STATUS_ICON | 🎯 |" >> completion-report.md
                    fi
                  fi
                done
              else
                # Fuzz testing enabled but no valid stats data
                {
                  echo "| Status | Details |"
                  echo "|--------|---------|"
                  echo "| **Fuzz Testing** | ✅ Enabled |"
                  echo "| **Execution** | ⚠️ No valid fuzz stats found - check job logs |"
                  echo "| **Platform** | Linux with primary Go version |"
                } >> completion-report.md
              fi
            else
              # Fuzz testing enabled but no stats files found
              {
                echo "| Status | Details |"
                echo "|--------|---------|"
                echo "| **Fuzz Testing** | ✅ Enabled |"
                echo "| **Execution** | ⚠️ No fuzz stats found - check job logs |"
                echo "| **Platform** | Linux with primary Go version |"
              } >> completion-report.md
            fi
          else
            # Fuzz testing is disabled
            {
              echo "| Status | Details |"
              echo "|--------|---------|"
              echo "| **Fuzz Testing** | ❌ Disabled |"
              echo "| **Configuration** | Set ENABLE_FUZZ_TESTING=true to enable |"
              echo "| **Target Platform** | Would run on Linux with primary Go version |"
            } >> completion-report.md
          fi

      # ————————————————————————————————————————————————————————————————
      # Process coverage statistics
      # ————————————————————————————————————————————————————————————————
      - name: 📊 Process Coverage Statistics
        id: process-coverage
        run: |
          # Process coverage statistics if available
          if compgen -G "coverage-stats-*.json" >/dev/null 2>&1; then
            # Check if we have valid coverage data before creating section
            HAS_COVERAGE_DATA=false
            VALID_COVERAGE_FILE=""
            for stats_file in coverage-stats-*.json; do
              if [ -f "$stats_file" ]; then
                COVERAGE_PERCENT=$(jq -r '.coverage_percent // "null"' "$stats_file")
                if [[ "$COVERAGE_PERCENT" != "null" ]] && [[ "$COVERAGE_PERCENT" != "N/A" ]] && [[ -n "$COVERAGE_PERCENT" ]]; then
                  HAS_COVERAGE_DATA=true
                  VALID_COVERAGE_FILE="$stats_file"
                  break
                fi
              fi
            done

            if [[ "$HAS_COVERAGE_DATA" == "true" ]] && [[ -n "$VALID_COVERAGE_FILE" ]]; then
              {
                echo "<br><br>"
                echo ""
                echo "### 📈 Code Coverage Report"
              } >> completion-report.md

              # Process the valid coverage file
              COVERAGE_PERCENT=$(jq -r '.coverage_percent // "N/A"' "$VALID_COVERAGE_FILE")
              PROCESSING_TIME=$(jq -r '.processing_time_seconds // "N/A"' "$VALID_COVERAGE_FILE")
              FILES_PROCESSED=$(jq -r '.files_processed // "N/A"' "$VALID_COVERAGE_FILE")
              BADGE_GENERATED=$(jq -r '.badge_generated // "false"' "$VALID_COVERAGE_FILE")
              PAGES_DEPLOYED=$(jq -r '.pages_deployed // "false"' "$VALID_COVERAGE_FILE")

              {
                echo "| Metric | Value |"
                echo "|--------|-------|"
                echo "| **Coverage Percentage** | $COVERAGE_PERCENT% |"
                echo "| **Processing Time** | ${PROCESSING_TIME}s |"
                echo "| **Files Processed** | $FILES_PROCESSED |"
                echo "| **Badge Generated** | $([ "$BADGE_GENERATED" == "true" ] && echo "✅ Yes" || echo "❌ No") |"
                echo "| **Pages Deployed** | $([ "$PAGES_DEPLOYED" == "true" ] && echo "✅ Yes" || echo "❌ No") |"
              } >> completion-report.md
            fi
          elif [[ "${{ env.ENABLE_CODE_COVERAGE }}" == "true" ]]; then
            {
              echo "<br><br>"
              echo ""
              echo "### 📈 Code Coverage Status"
              echo "| Status | Details |"
              echo "|--------|---------|"
              echo "| **System** | Internal GoFortress Coverage |"
              echo "| **Threshold** | ${{ env.GO_COVERAGE_THRESHOLD }}% minimum |"
              echo "| **Badge Style** | ${{ env.GO_COVERAGE_BADGE_STYLE }} |"
              echo "| **PR Comments** | $([ "${{ env.GO_COVERAGE_POST_COMMENTS }}" == "true" ] && echo "✅ Enabled" || echo "❌ Disabled") |"
              echo "| **Theme** | ${{ env.GO_COVERAGE_REPORT_THEME }} |"
            } >> completion-report.md
          fi

      # ————————————————————————————————————————————————————————————————
      # Generate Lines of Code Summary
      # ————————————————————————————————————————————————————————————————
      - name: 📊 Generate Lines of Code Summary
        id: process-loc
        run: |
          # Try to get LOC from stats files first
          LOC_FOUND=false
          TEST_FILES_COUNT=""
          GO_FILES_COUNT=""
          TOTAL_LOC=""
          LOC_DATE=""

          if compgen -G "*-stats-*.json" >/dev/null 2>&1; then
            echo "🔍 Looking for LOC data in stats files..."
            for stats_file in *-stats-*.json; do
              if [ -f "$stats_file" ]; then
                echo "📋 Checking $stats_file for LOC data..."
                TEST_FILES_COUNT=$(jq -r '.loc_test_files // "null"' "$stats_file")
                GO_FILES_COUNT=$(jq -r '.loc_go_files // "null"' "$stats_file")
                TOTAL_LOC=$(jq -r '.loc_total // "null"' "$stats_file")

                echo "  - Test Files: '$TEST_FILES_COUNT'"
                echo "  - Go Files: '$GO_FILES_COUNT'"
                echo "  - Total: '$TOTAL_LOC'"

                # Check if we have valid LOC data (not null and not empty string)
                if [[ "$TEST_FILES_COUNT" != "null" ]] && [[ "$TEST_FILES_COUNT" != "" ]] && \
                   [[ "$GO_FILES_COUNT" != "null" ]] && [[ "$GO_FILES_COUNT" != "" ]] && \
                   [[ "$TOTAL_LOC" != "null" ]] && [[ "$TOTAL_LOC" != "" ]]; then
                  LOC_DATE=$(jq -r '.loc_date // "unknown"' "$stats_file")
                  LOC_FOUND=true
                  echo "✅ Found valid LOC data in $stats_file: $TOTAL_LOC total lines"
                  break
                fi
              fi
            done
          fi

          # If not in stats, try to calculate directly using magex
          if [[ "$LOC_FOUND" == "false" ]]; then
            echo "🔍 LOC not found in stats, trying magex metrics:loc..."
            if command -v magex &> /dev/null; then
              LOC_OUTPUT=$(magex metrics:loc 2>&1 || true)
              if [[ -n "$LOC_OUTPUT" ]]; then
                echo "📋 Direct magex output:"
                echo "$LOC_OUTPUT"

                # Use same parsing as in test-suite workflow
                TEST_FILES_COUNT=$(echo "$LOC_OUTPUT" | grep "Test Files" | awk -F'|' '{print $3}' | tr -d ' ,' || echo "")
                GO_FILES_COUNT=$(echo "$LOC_OUTPUT" | grep "Go Files" | awk -F'|' '{print $3}' | tr -d ' ,' || echo "")
                TOTAL_LOC=$(echo "$LOC_OUTPUT" | grep "Total lines of code:" | awk -F':' '{print $2}' | tr -d ' ,' || echo "")
                LOC_DATE=$(date -u +"%Y-%m-%d")

                echo "📊 Directly parsed values:"
                echo "  - Test Files: '$TEST_FILES_COUNT'"
                echo "  - Go Files: '$GO_FILES_COUNT'"
                echo "  - Total: '$TOTAL_LOC'"

                if [[ -n "$TOTAL_LOC" ]] && [[ -n "$TEST_FILES_COUNT" ]] && [[ -n "$GO_FILES_COUNT" ]]; then
                  LOC_FOUND=true
                  echo "✅ Calculated LOC directly: $TOTAL_LOC total lines"
                else
                  echo "⚠️ Direct LOC calculation failed - incomplete data"
                fi
              else
                echo "⚠️ No output from magex metrics:loc"
              fi
            else
              echo "⚠️ magex not available in completion report environment"
            fi
          fi

          # Display LOC section if we have data
          if [[ "$LOC_FOUND" == "true" ]] && [[ -n "$TOTAL_LOC" ]]; then
            # Ensure no empty values in table
            DISPLAY_TEST_FILES="${TEST_FILES_COUNT:-N/A}"
            DISPLAY_GO_FILES="${GO_FILES_COUNT:-N/A}"
            DISPLAY_LOC_DATE="${LOC_DATE:-N/A}"
            DISPLAY_TOTAL="${TOTAL_LOC:-N/A}"

            # Double-check for empty strings and replace with N/A
            [[ -z "$DISPLAY_TEST_FILES" ]] && DISPLAY_TEST_FILES="N/A"
            [[ -z "$DISPLAY_GO_FILES" ]] && DISPLAY_GO_FILES="N/A"
            [[ -z "$DISPLAY_LOC_DATE" ]] && DISPLAY_LOC_DATE="N/A"
            [[ -z "$DISPLAY_TOTAL" ]] && DISPLAY_TOTAL="N/A"

            {
              echo "<br><br>"
              echo ""
              echo "### 📊 Lines of Code Summary"
              echo "| Type | Total Lines | Date |"
              echo "|------|-------------|------|"
              echo "| Test Files | $DISPLAY_TEST_FILES | $DISPLAY_LOC_DATE |"
              echo "| Go Files | $DISPLAY_GO_FILES | $DISPLAY_LOC_DATE |"
              echo ""
              echo "**Total lines of code: $DISPLAY_TOTAL**"
            } >> completion-report.md

            echo "✅ LOC section added to report with values: Test=$DISPLAY_TEST_FILES, Go=$DISPLAY_GO_FILES, Total=$DISPLAY_TOTAL"
          else
            echo "⚠️ No valid LOC data available to display"
            {
              echo "<br><br>"
              echo ""
              echo "### 📊 Lines of Code Summary"
              echo "| Status | Details |"
              echo "|--------|---------|"
              echo "| **Lines of Code** | ❌ Data not available |"
              echo "| **Reason** | magex metrics:loc not executed or parsing failed |"
            } >> completion-report.md
          fi

      # ————————————————————————————————————————————————————————————————
      # Generate job results summary
      # ————————————————————————————————————————————————————————————————
      - name: 🔧 Generate Job Results Summary
        id: job-results
        run: |
          {
            echo "<br><br>"
            echo ""
            echo "### ✅ Workflow Status Overview"
            echo "| Job | Status | Result |"
            echo "|-----|--------|--------|"
            echo "| 🎯 Setup Configuration | ${{ inputs.setup-result }} | $([ "${{ inputs.setup-result }}" = "success" ] && echo "✅" || echo "❌") |"
            echo "| 🪄 Test MAGE-X | ${{ inputs.test-magex-result }} | $([ "${{ inputs.test-magex-result }}" = "success" ] && echo "✅" || echo "❌") |"
            echo "| 🪝 Pre-commit Checks | ${{ inputs.pre-commit-result }} | $([ "${{ inputs.pre-commit-result }}" = "success" ] && echo "✅" || echo "❌") |"
            echo "| 🔒 Security Scans | ${{ inputs.security-result }} | $([ "${{ inputs.security-result }}" = "success" ] && echo "✅" || echo "❌") |"
            echo "| 📊 Code Quality | ${{ inputs.code-quality-result }} | $([ "${{ inputs.code-quality-result }}" = "success" ] && echo "✅" || echo "❌") |"
            echo "| 🧪 Test Suite | ${{ inputs.test-suite-result }} | $([ "${{ inputs.test-suite-result }}" = "success" ] && echo "✅" || echo "❌") |"
          } >> completion-report.md

          # Only show benchmarks row if it was attempted
          if [[ "${{ inputs.benchmarks-result }}" != "skipped" ]]; then
            echo "| 🏃 Benchmarks | ${{ inputs.benchmarks-result }} | $([ "${{ inputs.benchmarks-result }}" = "success" ] && echo "✅" || echo "❌") |" >> completion-report.md
          fi

          # Always show status-check result
          echo "| 🎯 All Tests Passed | ${{ inputs.status-check-result }} | $([ "${{ inputs.status-check-result }}" = "success" ] && echo "✅" || echo "❌") |" >> completion-report.md

          # Only show release row if it was attempted
          if [[ "${{ inputs.release-result }}" != "skipped" ]]; then
            echo "| 🚀 Release | ${{ inputs.release-result }} | $([ "${{ inputs.release-result }}" = "success" ] && echo "✅" || echo "❌") |" >> completion-report.md
          fi

          echo "" >> completion-report.md

          # Add release-specific information if this was a tag push
          if [[ "${{ github.ref }}" == refs/tags/v* ]]; then
            {
              echo ""
              echo "## 📦 Release Information"
            } >> completion-report.md

            if [[ "${{ inputs.release-result }}" == "success" ]]; then
              {
                echo "✅ Release ${{ github.ref_name }} created successfully!"
                echo "[View Release](https://github.com/${{ github.repository }}/releases/tag/${{ github.ref_name }})"
              } >> completion-report.md
            elif [[ "${{ inputs.release-result }}" == "skipped" ]]; then
              echo "⏭️ Release was skipped (likely due to test failures)" >> completion-report.md
            elif [[ "${{ inputs.release-result }}" == "failure" ]]; then
              echo "❌ Release creation failed - check logs for details" >> completion-report.md
            fi
            echo "" >> completion-report.md
          fi

      # ————————————————————————————————————————————————————————————————
      # Generate performance insights
      # ————————————————————————————————————————————————————————————————
      - name: 🚀 Generate Performance Insights
        id: performance-insights
        run: |
          TOTAL_DURATION=${{ steps.calculate-timing.outputs.total_duration }}
          TOTAL_MINUTES=${{ steps.calculate-timing.outputs.total_minutes }}
          TOTAL_SECONDS=${{ steps.calculate-timing.outputs.total_seconds }}

          {
            echo "<br><br>"
            echo ""
            echo "### 📊 Workflow Analytics & Insights"
          } >> completion-report.md

          # Overall timing insights
          if [[ $TOTAL_DURATION -gt 600 ]]; then
            echo "- ⚠️  **Warning**: Workflow took longer than 10 minutes (${TOTAL_MINUTES}m ${TOTAL_SECONDS}s)" >> completion-report.md
          elif [[ $TOTAL_DURATION -gt 300 && $TOTAL_DURATION -le 600 ]]; then
            echo "- ℹ️  Workflow completed in ${TOTAL_MINUTES}m ${TOTAL_SECONDS}s." >> completion-report.md
          elif [[ $TOTAL_DURATION -gt 180 && $TOTAL_DURATION -le 300 ]]; then
            echo "- 🎉  **Great Performance**: Workflow completed in under 5 minutes (${TOTAL_MINUTES}m ${TOTAL_SECONDS}s)!" >> completion-report.md
          elif [[ $TOTAL_DURATION -le 180 ]]; then
            echo "- 🚀  **Excellent Performance**: Workflow completed in under 3 minutes!" >> completion-report.md
          fi

          # Test-specific insights based on collected data
          if compgen -G "test-stats-*.json" >/dev/null 2>&1; then
            TOTAL_TESTS=${{ steps.process-tests.outputs.total_tests || 0 }}
            TOTAL_FAILURES=${{ steps.process-tests.outputs.total_failures || 0 }}

            # Calculate test-specific metrics
            FULL_MODE_SUITES=0
            FAILURES_ONLY_SUITES=0
            ESTIMATED_SAVINGS_MB=0

            for stats_file in test-stats-*.json; do
              if [ -f "$stats_file" ]; then
                MODE=$(jq -r '.test_mode // "unknown"' "$stats_file")
                OUTPUT_SIZE=$(jq -r '.output_size_bytes // 0' "$stats_file")

                if [[ "$MODE" == "FULL" ]]; then
                  FULL_MODE_SUITES=$((FULL_MODE_SUITES + 1))
                elif [[ "$MODE" == "FAILURES_ONLY" ]]; then
                  FAILURES_ONLY_SUITES=$((FAILURES_ONLY_SUITES + 1))
                  # Estimate savings (assume FULL mode would be 5-10x larger)
                  ESTIMATED_FULL_SIZE=$((OUTPUT_SIZE * 7))
                  SAVINGS=$((ESTIMATED_FULL_SIZE - OUTPUT_SIZE))
                  ESTIMATED_SAVINGS_MB=$((ESTIMATED_SAVINGS_MB + SAVINGS / 1048576))
                fi
              fi
            done

            echo "- **Test Strategy**: $FULL_MODE_SUITES suite(s) used FULL output, $FAILURES_ONLY_SUITES used efficient FAILURES_ONLY" >> completion-report.md

            if [[ $TOTAL_FAILURES -gt 0 ]] && [[ $TOTAL_TESTS -gt 0 ]]; then
              FAILURE_RATE=$(( (TOTAL_FAILURES * 10000) / TOTAL_TESTS ))  # Calculate percentage * 100 for precision
              FAILURE_RATE_DISPLAY=$(( FAILURE_RATE / 100 )).$(( FAILURE_RATE % 100 ))
              echo "- **Failure Rate**: $FAILURE_RATE_DISPLAY% ($TOTAL_FAILURES/$TOTAL_TESTS tests)" >> completion-report.md

              # Find most affected package
              MOST_AFFECTED_PACKAGE=""
              MAX_FAILURES=0
              for stats_file in test-stats-*.json; do
                if [ -f "$stats_file" ]; then
                  FAILURE_DETAILS=$(jq -r '.failure_details // null' "$stats_file")
                  if [[ "$FAILURE_DETAILS" != "null" ]] && [[ "$FAILURE_DETAILS" != "[]" ]]; then
                    # Extract package with most failures
                    PACKAGE_FAILURES=$(echo "$FAILURE_DETAILS" | jq -r 'group_by(.Package) | map({package: .[0].Package, count: length}) | max_by(.count)' 2>/dev/null)
                    if [[ "$PACKAGE_FAILURES" != "null" ]]; then
                      PKG_NAME=$(echo "$PACKAGE_FAILURES" | jq -r '.package | split("/") | .[-1] // .[-2] // .')
                      PKG_COUNT=$(echo "$PACKAGE_FAILURES" | jq -r '.count')
                      if [[ $PKG_COUNT -gt $MAX_FAILURES ]]; then
                        MAX_FAILURES=$PKG_COUNT
                        MOST_AFFECTED_PACKAGE="$PKG_NAME"
                      fi
                    fi
                  fi
                fi
              done

              if [[ -n "$MOST_AFFECTED_PACKAGE" ]] && [[ $MAX_FAILURES -gt 0 ]]; then
                echo "- **Most Affected Package**: $MOST_AFFECTED_PACKAGE ($MAX_FAILURES failures)" >> completion-report.md
              fi
            elif [[ $TOTAL_TESTS -gt 0 ]]; then
              echo "- **Test Success**: All $TOTAL_TESTS tests passed! 🎉" >> completion-report.md
            fi

            if [[ $ESTIMATED_SAVINGS_MB -gt 0 ]]; then
              echo "- **Output Efficiency**: Saved approximately ${ESTIMATED_SAVINGS_MB}MB by using FAILURES_ONLY mode" >> completion-report.md
            fi
          fi

          # Standard insights
          {
            echo "- **Parallel Jobs**: Multiple jobs ran in parallel to optimize execution time"
            echo "- **Matrix Strategy**: Tests ran across $(echo '${{ inputs.test-matrix }}' | jq '.include | length') configurations"
          } >> completion-report.md

          if [ "${{ env.ENABLE_VERBOSE_TEST_OUTPUT }}" != "true" ]; then
            echo "- **Verbose Output**: Disabled to speed up test execution" >> completion-report.md
          else
            echo "- **Verbose Output**: Enabled for detailed test logs" >> completion-report.md
          fi

          # Add failure analysis if any job failed
          FAILED_JOBS=""
          [ "${{ inputs.setup-result }}" != "success" ] && [ "${{ inputs.setup-result }}" != "skipped" ] && FAILED_JOBS="${FAILED_JOBS}Setup Configuration, "
          [ "${{ inputs.test-magex-result }}" != "success" ] && [ "${{ inputs.test-magex-result }}" != "skipped" ] && FAILED_JOBS="${FAILED_JOBS}Test MAGE-X, "
          [ "${{ inputs.pre-commit-result }}" != "success" ] && [ "${{ inputs.pre-commit-result }}" != "skipped" ] && FAILED_JOBS="${FAILED_JOBS}Pre-commit Checks, "
          [ "${{ inputs.security-result }}" != "success" ] && [ "${{ inputs.security-result }}" != "skipped" ] && FAILED_JOBS="${FAILED_JOBS}Security Scans, "
          [ "${{ inputs.code-quality-result }}" != "success" ] && [ "${{ inputs.code-quality-result }}" != "skipped" ] && FAILED_JOBS="${FAILED_JOBS}Code Quality, "
          [ "${{ inputs.test-suite-result }}" != "success" ] && [ "${{ inputs.test-suite-result }}" != "skipped" ] && FAILED_JOBS="${FAILED_JOBS}Test Suite, "
          [ "${{ inputs.benchmarks-result }}" != "success" ] && [ "${{ inputs.benchmarks-result }}" != "skipped" ] && FAILED_JOBS="${FAILED_JOBS}Benchmarks, "
          [ "${{ inputs.status-check-result }}" != "success" ] && [ "${{ inputs.status-check-result }}" != "skipped" ] && FAILED_JOBS="${FAILED_JOBS}Status Check, "
          [ "${{ inputs.release-result }}" != "success" ] && [ "${{ inputs.release-result }}" != "skipped" ] && FAILED_JOBS="${FAILED_JOBS}Release, "

          if [ -n "$FAILED_JOBS" ]; then
            FAILED_JOBS=${FAILED_JOBS%, }  # Remove trailing comma
            {
              echo "<br><br>"
              echo ""
              echo "### ⚠️ Failed Components"
              echo "The following jobs did not complete successfully:"
              echo "- ${FAILED_JOBS}"
            } >> completion-report.md
          fi

      # ————————————————————————————————————————————————————————————————
      # Finalize completion report
      # ————————————————————————————————————————————————————————————————
      - name: ✅ Finalize Completion Report
        id: finalize-report
        run: |
          # Add professional footer before finalizing
          {
            echo "<br><br>"
            echo ""
            echo "---"
            echo "🎯 **Workflow completed** at $(date -u +"%H:%M:%S UTC")"
            echo ""
            echo "_GoFortress CI/CD Pipeline - Built Strong. Tested Harder._"
          } >> completion-report.md

          # Write the final report to GitHub Step Summary
          cat completion-report.md >> $GITHUB_STEP_SUMMARY
          echo "✅ Completion report generated successfully"
